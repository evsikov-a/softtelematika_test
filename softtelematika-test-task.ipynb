{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom tqdm.notebook import tqdm\n\nimport numpy as np\nimport pandas as pd\nimport albumentations as A\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nimport keras\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model","metadata":{"execution":{"iopub.status.busy":"2021-06-13T09:53:10.351705Z","iopub.execute_input":"2021-06-13T09:53:10.351922Z","iopub.status.idle":"2021-06-13T09:53:19.459005Z","shell.execute_reply.started":"2021-06-13T09:53:10.351899Z","shell.execute_reply":"2021-06-13T09:53:19.45832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install efficientnet\nimport efficientnet.tfkeras","metadata":{"execution":{"iopub.status.busy":"2021-06-13T09:53:19.482995Z","iopub.execute_input":"2021-06-13T09:53:19.483202Z","iopub.status.idle":"2021-06-13T09:53:27.145929Z","shell.execute_reply.started":"2021-06-13T09:53:19.483179Z","shell.execute_reply":"2021-06-13T09:53:27.145148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/qubvel/segmentation_models\nimport segmentation_models as sm","metadata":{"execution":{"iopub.status.busy":"2021-06-13T09:53:27.14757Z","iopub.execute_input":"2021-06-13T09:53:27.147919Z","iopub.status.idle":"2021-06-13T09:53:36.488978Z","shell.execute_reply.started":"2021-06-13T09:53:27.147873Z","shell.execute_reply":"2021-06-13T09:53:36.488235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = \"../input/cityscapes-dataset-1000/dataset_square1024_1000items/dataset_square1024_1000items/\"\nIMAGES_DIR = DATA_DIR + \"images\"\nMASKS_DIR = DATA_DIR + \"masks\"","metadata":{"execution":{"iopub.status.busy":"2021-06-13T09:53:36.492093Z","iopub.execute_input":"2021-06-13T09:53:36.492384Z","iopub.status.idle":"2021-06-13T09:53:36.496941Z","shell.execute_reply.started":"2021-06-13T09:53:36.492345Z","shell.execute_reply":"2021-06-13T09:53:36.49606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_ids = os.listdir(IMAGES_DIR)\nids_train, ids_val = train_test_split(all_ids, test_size = 0.15, random_state = 1)\nassert set(all_ids) == set(ids_train + ids_val)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T09:53:36.498152Z","iopub.execute_input":"2021-06-13T09:53:36.498414Z","iopub.status.idle":"2021-06-13T09:53:36.582499Z","shell.execute_reply.started":"2021-06-13T09:53:36.49838Z","shell.execute_reply":"2021-06-13T09:53:36.581731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataloader and utility functions","metadata":{}},{"cell_type":"code","source":"# helper function for data visualization\ndef visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()\n    \n# helper function for data visualization    \ndef denormalize(x):\n    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n    x_max = np.percentile(x, 98)\n    x_min = np.percentile(x, 2)    \n    x = (x - x_min) / (x_max - x_min)\n    x = x.clip(0, 1)\n    return x\n    \nclass Dataset():\n    \n    \"\"\"\n    Read images, apply augmentation and preprocessing transformations.\n    \n    Args:\n        images_dir: path to images folder\n        masks_dir: path to segmentation masks folder\n        class_values: values of classes to extract from segmentation mask\n        ids: list of image/mask names to use in this dataset, if not given all the files in given directories would be used\n        augmentation: data transfromation pipeline\n        preprocessing: data preprocessing \n    \"\"\"\n    \n    CLASSES = ['ground', 'road', 'sidewalk', 'building', 'fence',\n              'traffic light', 'traffic sign', 'vegetation', 'sky',\n              'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle']\n    \n    def __init__(self, images_dir, masks_dir, ids = None, augmentation = None, preprocessing = None):\n\n        if ids != None:\n            self.ids = ids\n        else:\n            self.ids = os.listdir(images_dir)\n            \n        self.images_paths = [os.path.join(images_dir, path) for path in self.ids]\n        self.masks_paths_by_class = {cls: [os.path.join(masks_dir, cls, path) for path in self.ids] for cls in self.CLASSES}\n        \n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    \n    def __getitem__(self, i):\n        image = cv2.cvtColor(cv2.imread(self.images_paths[i]), cv2.COLOR_BGR2RGB)\n        \n        # array of one-class masks with boolean values\n        masks = []\n        \n        for cls in self.CLASSES:\n            # rgb mask to boolean \n            mask = cv2.imread(self.masks_paths_by_class[cls][i])\n            mask = (mask[:, :, 0:3] == [255,255,255]).all(2)\n            masks.append(mask)\n        \n        # Combine one-class masks to multi-class mask where each pixel is represented by one-hot vector\n        mask = np.stack(masks, axis = -1).astype(\"float\")\n\n        # add background if mask is not binary\n        if mask.shape[-1] != 1:\n            background = 1 - mask.sum(axis=-1, keepdims=True)\n            mask = np.concatenate((mask, background), axis=-1)\n            \n        # augmentation\n        if self.augmentation != None: \n            sample = self.augmentation(image = image, mask = mask)\n            image, mask = sample[\"image\"], sample[\"mask\"]\n    \n        # preprocessing\n        if self.preprocessing != None: \n            image = self.preprocessing(image = image)[\"image\"]\n        \n        return image, mask\n    \n    def __len__(self):\n        return len(self.ids)\n\n    \nclass Dataloader(keras.utils.Sequence):\n    \n    \"\"\"\n    Wrapper for dataset instance which samples batches of images\n    \n    Args:\n        dataset: Dataset class instance for image loading and preprocessing\n        batch_size: the number of image-mask pairs sampled in each batch\n        shuffle: if shuffle the dataset after each epoch\n    \"\"\"\n    \n    def __init__(self, dataset, batch_size = 1, shuffle = False):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.indexes = np.arange(len(dataset))\n        \n        self.on_epoch_end()\n        \n    def __getitem__(self, i):\n        start = self.batch_size * i\n        stop = self.batch_size * (i + 1)\n        data = []\n        \n        for j in range(start, stop):\n            data.append(self.dataset[j])\n        \n        # List of two np darrays: with images and with masks\n        batch = [np.stack(samples, axis = 0) for samples in zip(*data)]\n        \n        return batch\n        \n    def __len__(self):\n        return len(self.indexes) // self.batch_size\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            self.indexes = np.random.permutation(self.indexes)    ","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:28:20.622626Z","iopub.execute_input":"2021-06-12T16:28:20.622952Z","iopub.status.idle":"2021-06-12T16:28:20.76655Z","shell.execute_reply.started":"2021-06-12T16:28:20.622913Z","shell.execute_reply":"2021-06-12T16:28:20.76567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Augmentations were selected so that to maximally close mosreg dataset to google maps images distribution\ndef get_training_augmentation():\n    \"\"\"\n    Return: albumentations.Compose with augmentations\n    \"\"\"\n    # We don't care about the rotation of aerial image\n    train_transform = [A.Flip(p=0.75)]\n    \n    return A.Compose(train_transform)\n\n# Image and mask increasing\ndef get_preprocessing(preprocessing_fn = None):\n    \"\"\"\n    Construct preprocessing transform\n    \n    Arguments:\n        preprocessing_fn (callbale): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \n    \"\"\"\n    preprocessing_transform = [A.Lambda(image=preprocessing_fn)]\n    \n    return A.Compose(preprocessing_transform)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-13T09:53:36.584156Z","iopub.execute_input":"2021-06-13T09:53:36.584624Z","iopub.status.idle":"2021-06-13T09:53:36.590834Z","shell.execute_reply.started":"2021-06-13T09:53:36.584565Z","shell.execute_reply":"2021-06-13T09:53:36.590098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization of dataset","metadata":{}},{"cell_type":"code","source":"def visualize_image_with_bin_masks(image, mask):\n    visualize(image = image, \n              cloud_dense = mask[:, :, 0], \n              cloud_translucent = mask[:, :, 1], \n              shadow = mask[:, :, 2], \n              snow = mask[:, :, 3])\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:28:20.777174Z","iopub.execute_input":"2021-06-12T16:28:20.77772Z","iopub.status.idle":"2021-06-12T16:28:20.784292Z","shell.execute_reply.started":"2021-06-12T16:28:20.777661Z","shell.execute_reply":"2021-06-12T16:28:20.783607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Setup","metadata":{}},{"cell_type":"code","source":"LR = 0.0001\nEPOCHS = 20\nBATCH_SIZE = 1\nINPUT_SHAPE = (1024, 1024, 3)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T09:53:48.651708Z","iopub.execute_input":"2021-06-13T09:53:48.651984Z","iopub.status.idle":"2021-06-13T09:53:48.656667Z","shell.execute_reply.started":"2021-06-13T09:53:48.651956Z","shell.execute_reply":"2021-06-13T09:53:48.655655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BACKBONE = \"efficientnetb5\"\nCLASSES = ['ground', 'road', 'sidewalk', 'building', 'fence',\n              'traffic light', 'traffic sign', 'vegetation', 'sky',\n              'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle']\n\npreprocess_input = sm.get_preprocessing(BACKBONE)\n\n# define network parameters\nn_classes = 1 if len(CLASSES) == 1 else (len(CLASSES) + 1)  # case for binary and multiclass segmentation\nactivation = 'sigmoid' if n_classes == 1 else 'softmax'\n\n# create model and freeze encoder weights to prevent spoiling them by high dirst gradients\nmodel = sm.Unet(BACKBONE, encoder_weights=\"imagenet\", input_shape = INPUT_SHAPE, classes = n_classes, activation = activation)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T09:53:49.402946Z","iopub.execute_input":"2021-06-13T09:53:49.40321Z","iopub.status.idle":"2021-06-13T09:54:01.393131Z","shell.execute_reply.started":"2021-06-13T09:53:49.40318Z","shell.execute_reply":"2021-06-13T09:54:01.392436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load weights to continue training\nmodel.load_weights(\"../input/cityscapes-models/cityscapes_1000_20ep.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-06-13T09:54:22.229758Z","iopub.execute_input":"2021-06-13T09:54:22.230028Z","iopub.status.idle":"2021-06-13T09:54:25.613235Z","shell.execute_reply.started":"2021-06-13T09:54:22.23Z","shell.execute_reply":"2021-06-13T09:54:25.612484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset(IMAGES_DIR, MASKS_DIR, ids = ids_train, augmentation = None, preprocessing = get_preprocessing(preprocess_input))\nval_dataset = Dataset(IMAGES_DIR, MASKS_DIR, ids = ids_val, preprocessing = get_preprocessing(preprocess_input))\n\ntrain_dataloader = Dataloader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True)\nval_dataloader = Dataloader(dataset = val_dataset, batch_size = 1, shuffle = False)\n\n# check shapes for errors\nassert train_dataloader[0][0].shape == (BATCH_SIZE, INPUT_SHAPE[0], INPUT_SHAPE[1], INPUT_SHAPE[2])\nassert train_dataloader[0][1].shape == (BATCH_SIZE, INPUT_SHAPE[0], INPUT_SHAPE[1], n_classes)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:28:37.648412Z","iopub.execute_input":"2021-06-12T16:28:37.648676Z","iopub.status.idle":"2021-06-12T16:28:39.253655Z","shell.execute_reply.started":"2021-06-12T16:28:37.648642Z","shell.execute_reply":"2021-06-12T16:28:39.2529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# define callbacks for learning rate scheduling and best checkpoints saving\ncallbacks = [\n#     keras.callbacks.ModelCheckpoint('./clean_mosaic_unet_b4_40ep_dataset_1_best.hdf5', save_best_only=True, mode = \"min\"),\n    keras.callbacks.ReduceLROnPlateau(verbose = 1),\n]\n\n# define optimizer\noptim = keras.optimizers.Adam(LR)\ntotal_loss = sm.losses.binary_focal_dice_loss  if n_classes == 1 else sm.losses.categorical_focal_dice_loss \nmetrics = [sm.metrics.IOUScore(), sm.metrics.FScore()]\n\n# compile keras model with defined optimozer, loss and metrics\nmodel.compile(optim, total_loss, metrics)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:28:39.255002Z","iopub.execute_input":"2021-06-12T16:28:39.255279Z","iopub.status.idle":"2021-06-12T16:28:39.357542Z","shell.execute_reply.started":"2021-06-12T16:28:39.255244Z","shell.execute_reply":"2021-06-12T16:28:39.356601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit_generator(\n    train_dataloader, \n    steps_per_epoch=len(train_dataloader), \n    epochs=EPOCHS, \n    callbacks=callbacks,\n    validation_data=val_dataloader, \n    validation_steps=len(val_dataloader),\n)\n\nmodel.save(\"cityscapes_1000_40ep.hdf5\")\nmodel.save_weights(\"cityscapes_1000_40ep.h5\")\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:28:39.359431Z","iopub.execute_input":"2021-06-12T16:28:39.359715Z","iopub.status.idle":"2021-06-12T21:07:20.522548Z","shell.execute_reply.started":"2021-06-12T16:28:39.359658Z","shell.execute_reply":"2021-06-12T21:07:20.520505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"cityscapes_1000_35ep.hdf5\")\nmodel.save_weights(\"cityscapes_1000_35ep.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:07:31.802566Z","iopub.execute_input":"2021-06-12T21:07:31.802882Z","iopub.status.idle":"2021-06-12T21:07:35.02177Z","shell.execute_reply.started":"2021-06-12T21:07:31.802849Z","shell.execute_reply":"2021-06-12T21:07:35.020976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Video processing","metadata":{}},{"cell_type":"code","source":"def video_processing(file_path: str, output_name: str, model, colors_to_classes: dict, total_frames: int = None) -> None:\n    \"\"\"\n    Open videofile, make segmentation of every frame and generate new video with contours of predicted classes\n    \n    Args:\n        file_path: full path to videofile\n        output_name: name of final video\n        model: the model which would be used for multiclass semantic segmentation\n        colors_to_classes: dictionary with colors of every class\n        total_frames: (optional) total number of frames in final video\n    \"\"\"\n    \n    def frame_processing(frame: np.ndarray, model, colors_to_classes: dict) -> np.ndarray:\n        \"\"\"\n        Transform frame shape to shape of model's input, predict masks of classes, drow contours and transform to original shape\n\n        Args:\n            frame: a frame of 720p video as np.ndarray\n            model: the model which would be used for multiclass semantic segmentation\n            colors_to_classes: dictionary with colors of every class\n        Returns:\n            processed_frame: frame of new 720p video, with drew contours\n        \"\"\"\n        def predict_multiclass_mask(model, image: np.ndarray, image_preprocessing=get_preprocessing(preprocess_input)) -> np.ndarray:\n            \"\"\"\n            Arguments:\n            model: the model which would be used for multiclass semantic segmentation\n            image: an RGB image as numpy.darray of shape (H, W, 3), based on which masks are predicted \n            image_preprocessing: function for preprocessing rgb image e.g. normalization, padding\n\n\n            Returns: \n            mask: numpy.darray of shape (H, W), where every pixel has value from 0 to 4\n            \"\"\"\n            # Convert RGB image to array if needed\n            image = np.asarray(image)\n\n            # Preprocess rgb image if needed\n            if image_preprocessing != None:\n                image = image_preprocessing(image = image)[\"image\"]\n\n            if len(image.shape) == 3:\n            # input to the model should be of shape: (1, H, W, 3)\n                image = np.expand_dims(image, 0)\n\n            # Prediction is of shape: (1, H, W, len(classes)), last dimension is one-hot vector with probability of pixel being of the class predicted\n            # Prediction array is squeezed to (H, W, len(classes))\n            prediction = model.predict(image).squeeze()\n\n            #transform one-hot vector to index of class with max probability\n            #masks_idxs is of shape (H, W)\n            mask = np.argmax(prediction, axis = 2)\n\n            return mask\n        \n        def masks_dict_from_multiclass_mask(mask, classes = CLASSES,\n                                    class_pixel_value=np.array([255,255,255]), background_pixel_value=np.array([0,0,0])):\n            \"\"\"\n            Arguments:\n                mask: numpy.darray of shape (H, W), where every pixel has value from 0 to 4\n                class_pixel_value: RGB color of the class\n\n            Returns: \n                mask_dict: dictionary with classes as keys and masks as numpy.darray of shape (H, W, 3) for every class\n            \"\"\"\n            #Create dictionary for masks of every class\n            masks_dict = {}\n\n            idx = 0\n            for class_name in classes:\n                masks_dict[class_name] = np.where(mask[..., None] == idx, class_pixel_value, background_pixel_value).astype(np.uint8)\n                idx += 1\n\n            return masks_dict \n        \n        def draw_contours_from_multiclass_masks(image: np.ndarray, class_to_mask_dict: dict, colors_to_classes: dict) -> np.ndarray:\n            '''\n            Arguments:\n                image: an RGB image as numpy.darray of shape (H, W, 3)\n                class_to_mask_dict: dictionary with classes as keys and masks as numpy.darray of shape (H, W, 3) for every class\n                colors: list of tuples of format (r, g, b), where r,g,b integers from 0 to 255\n\n            Returns:\n                image: an RGB image with contours as numpy.darray of shape (H, W, 3) \n            '''\n\n            def draw_contours_from_mask_on_image(image: np.ndarray, mask: np.ndarray, color: tuple) -> np.ndarray:\n                '''\n                Arguments:\n                    image: an RGB image as numpy.darray of shape (H, W, 3)\n                    mask: an RGB image as numpy.darray of shape (H, W, 3)\n                    color: tuple of format (r, g, b), where r,g,b integers from 0 to 255\n\n                Returns:\n                    image: an RGB image with contours as numpy.darray of shape (H, W, 3) \n                '''\n\n                mask = mask.astype(np.uint8)[:, :, 1]\n                contours, _ = cv2.findContours(mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n                for cnt in contours:\n                    cv2.drawContours(image, [cnt], -1, color, -1)\n                return image\n    \n            for class_name in colors_to_classes.keys():\n                color = colors_to_classes[class_name]\n                if color is None:\n                    pass\n                else:\n                    draw_contours_from_mask_on_image(image, class_to_mask_dict[class_name], color)\n\n            return image\n        \n        shape_for_model_input = (1024, 576)\n        pad_width = ((224,224), (0,0), (0,0))\n        video_resolution = (1280, 720)\n        alpha = 0.5  # Transparency factor.\n\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        downscaled = cv2.resize(frame, shape_for_model_input)\n        downscaled_and_padded = np.pad(downscaled, pad_width)\n        multiclass_mask = predict_multiclass_mask(model, downscaled_and_padded)\n        masks = masks_dict_from_multiclass_mask(multiclass_mask)\n        downscaled_with_contours = draw_contours_from_multiclass_masks(downscaled_and_padded, masks, colors_to_classes)[224:224+576,:,:]\n        frame_with_contours = cv2.resize(downscaled_with_contours, video_resolution)\n        processed_frame = cv2.cvtColor(frame_with_contours, cv2.COLOR_RGB2BGR)\n        \n        processed_frame = cv2.addWeighted(processed_frame, alpha, frame, 1 - alpha, 0)\n\n        return processed_frame\n    \n    video = cv2.VideoCapture(file_path)\n    \n    width, height = (int(video.get(cv2.CAP_PROP_FRAME_WIDTH)), int(video.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n    fps = int(video.get(cv2.CAP_PROP_FPS))\n    \n    # Define codec and create VideoWriter\n    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n    out = cv2.VideoWriter()\n    output_file_name = f\"{output_name}.mp4\"\n    out.open(output_file_name, fourcc, fps, (width, height), True) \n    \n    if total_frames is not None:\n        i = 0\n        pbar = tqdm()\n        pbar.reset(total = total_frames)\n    \n    while video.isOpened():\n        _, frame = video.read()\n        final_frame = frame_processing(frame, model, colors_to_classes)\n        out.write(final_frame)\n        \n        if total_frames is not None:\n            i += 1\n            pbar.update()\n            if i == total_frames :\n                break\n\n    video.release()\n    out.release()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T09:54:32.47577Z","iopub.execute_input":"2021-06-13T09:54:32.476058Z","iopub.status.idle":"2021-06-13T09:54:32.507945Z","shell.execute_reply.started":"2021-06-13T09:54:32.476027Z","shell.execute_reply":"2021-06-13T09:54:32.507115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors_to_classes = {\n    'ground': None,\n    'sky': (0,0,255),\n    'road': (255,0,255), \n    'sidewalk': (0,255,255),\n    'building': (255,127,0),\n    'fence': (128,128,128),\n    'vegetation': (0,255,0),\n    'car': (255,0,0),\n    'traffic light': (255,255,0),\n    'traffic sign': (139,0,139),\n    'truck': (255,0,0),\n    'bus': (255,0,0),\n    'train': None,\n    'motorcycle': (255,105,180),\n    'bicycle': (255,105,180)\n}\nfile_path = '../input/test-video/test_video_msk_720.mp4'\nfinal_video_name = 'test_msk_full'\ntotal_frames = 12600\n","metadata":{"execution":{"iopub.status.busy":"2021-06-13T11:18:13.441403Z","iopub.execute_input":"2021-06-13T11:18:13.441788Z","iopub.status.idle":"2021-06-13T11:18:13.45598Z","shell.execute_reply.started":"2021-06-13T11:18:13.441747Z","shell.execute_reply":"2021-06-13T11:18:13.45515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"video_processing(file_path, final_video_name, model, colors_to_classes, total_frames)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T11:18:38.63881Z","iopub.execute_input":"2021-06-13T11:18:38.639075Z","iopub.status.idle":"2021-06-13T13:34:11.120304Z","shell.execute_reply.started":"2021-06-13T11:18:38.639048Z","shell.execute_reply":"2021-06-13T13:34:11.11948Z"},"trusted":true},"execution_count":null,"outputs":[]}]}